{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we consider a one-dimensional Gaussian distribution at the particle level and two Gaussian distributions at the detector level. The data are generated as follows:\n",
    "\\begin{aligned}\n",
    "    Y_{1} &= X + Z_{1}, \\\\\n",
    "    Y_{2} &= X + Z_{2},\n",
    "\\end{aligned}\n",
    "where $X\\sim\\mathcal{N}(\\mu,\\sigma^2), Z_{1}\\sim\\mathcal{N}(0,1), Z_{2}\\sim\\mathcal{N}(0,\\theta^2)$. Here, $\\theta$ is the nuisance parameter, which only affects the second coordinate of the detector-level data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "# POF functions\n",
    "import utils\n",
    "import profile_omnifold as pof\n",
    "\n",
    "dvc = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {dvc} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Gaussian Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the nuisance parameter\n",
    "theta = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smearing kernel for the MC data\n",
    "mu_kernel_mc = 0\n",
    "sigma1_kernel_mc = 1\n",
    "sigma2_kernel_mc = 1\n",
    "\n",
    "# smearing kernel for the experimental data\n",
    "mu_kernel_data = 0\n",
    "sigma1_kernel_data = 1\n",
    "sigma2_kernel_data = theta\n",
    "\n",
    "def k_mc(y,x):\n",
    "    return norm.pdf(y[:,0]-x[:,0],loc=mu_kernel_mc,scale=sigma1_kernel_mc)*norm.pdf(y[:,1]-x[:,0],loc=mu_kernel_mc,scale=sigma2_kernel_mc)\n",
    "\n",
    "def k_data(y,x):\n",
    "    return norm.pdf(y[:,0]-x[:,0],loc=mu_kernel_data,scale=sigma1_kernel_data)*norm.pdf(y[:,1]-x[:,0],loc=mu_kernel_data,scale=sigma2_kernel_data)\n",
    "\n",
    "# reweighting function w parametrized by theta\n",
    "# w(x,y,theta) = p(y|x,theta)/p(y|x,mc)\n",
    "def w_func(x,y,theta):\n",
    "    return norm.pdf(y[:,0]-x[:,0],loc=mu_kernel_data,scale=sigma1_kernel_data)*norm.pdf(y[:,1]-x[:,0],loc=mu_kernel_data,scale=theta)/k_mc(y,x)\n",
    "\n",
    "# derivative of w with respect to theta\n",
    "def w_func_derivative(x,y,theta):\n",
    "    return ((y[:,1]-x[:,0])**2/(theta**3)-1/theta)*w_func(x,y,theta)\n",
    "\n",
    "# true reweighting function on the MC response kernel (i.e. k_data/k_mc)\n",
    "def w_true(x,y):\n",
    "    return k_data(y,x)/k_mc(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Data (Nature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the parameters for the particle-level distribution\n",
    "mu_data = 0.8\n",
    "sigma_data = 1\n",
    "\n",
    "def px(x):\n",
    "    return norm.pdf(x,loc=mu_data,scale=sigma_data)\n",
    "def py1(y):\n",
    "    return norm.pdf(y,loc=mu_data+mu_kernel_data,scale=np.sqrt(sigma_data**2+sigma1_kernel_data**2))\n",
    "def py2(y):\n",
    "    return norm.pdf(y,loc=mu_data+mu_kernel_data,scale=np.sqrt(sigma_data**2+sigma2_kernel_data**2))\n",
    "\n",
    "\n",
    "# sample from the experimental distribution\n",
    "Ndata = 100000\n",
    "x_data = np.random.normal(loc=mu_data,scale=sigma_data,size=Ndata).reshape(-1,1)\n",
    "y_data1 = np.random.normal(loc=x_data[:,0]+mu_kernel_data,scale=sigma1_kernel_data,size=Ndata).reshape(-1,1)\n",
    "y_data2 = np.random.normal(loc=x_data[:,0]+mu_kernel_data,scale=sigma2_kernel_data,size=Ndata).reshape(-1,1)\n",
    "y_data = np.hstack([y_data1, y_data2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Data (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the parameters for the particle-level distribution\n",
    "mu_mc = 0\n",
    "sigma_mc = 1\n",
    "\n",
    "\n",
    "def qx(x):\n",
    "    return norm.pdf(x,loc=mu_mc,scale=sigma_mc)\n",
    "def qy1(y):\n",
    "    return norm.pdf(y,loc=mu_mc+mu_kernel_mc,scale=np.sqrt(sigma_mc**2+sigma1_kernel_mc**2))\n",
    "def qy2(y):\n",
    "    return norm.pdf(y,loc=mu_mc+mu_kernel_mc,scale=np.sqrt(sigma_mc**2+sigma2_kernel_mc**2))\n",
    "\n",
    "\n",
    "# sample from the MC distribution\n",
    "Nsim = 100000\n",
    "x_mc = np.random.normal(loc=mu_mc,scale=sigma_mc,size=Nsim).reshape(-1,1)\n",
    "y_mc1 = np.random.normal(loc=x_mc[:,0]+mu_kernel_mc,scale=sigma1_kernel_mc,size=Nsim).reshape(-1,1)\n",
    "y_mc2 = np.random.normal(loc=x_mc[:,0]+mu_kernel_mc,scale=sigma2_kernel_mc,size=Nsim).reshape(-1,1)\n",
    "y_mc = np.hstack([y_mc1,y_mc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot both experimental and MC distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharey=True, sharex=True, figsize=(15, 6))\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=x_data[:, 0], ax=ax[0],\n",
    "    color=\"black\", linestyle=\"-\", linewidth=2, bw_adjust=2,\n",
    "    label=f\"Experiment\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=x_mc[:, 0], ax=ax[0],\n",
    "    color=\"tab:blue\", linestyle=\":\", linewidth=2, bw_adjust=2, \n",
    "    label=f\"MC Simulation\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=y_data[:, 0], ax=ax[1],\n",
    "    color=\"black\", linestyle=\"-\", linewidth=2, bw_adjust=2,\n",
    "    label=f\"Experiment\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=y_mc[:, 0], ax=ax[1],\n",
    "    color=\"tab:blue\", linestyle=\":\", linewidth=2, bw_adjust=2, \n",
    "    label=f\"MC Simulation\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=y_data[:, 1], ax=ax[2],\n",
    "    color=\"black\", linestyle=\"-\", linewidth=2, bw_adjust=2,\n",
    "    label=f\"Experiment\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=y_mc[:, 1], ax=ax[2],\n",
    "    color=\"tab:blue\", linestyle=\":\", linewidth=2, bw_adjust=2, \n",
    "    label=f\"MC Simulation\"\n",
    ")\n",
    "\n",
    "\n",
    "ax[2].legend(loc=\"best\", fontsize=15)\n",
    "ax[0].set_xlim(-6, 6)\n",
    "ax[0].set_xlabel(r\"$X$\", fontsize=20)\n",
    "ax[1].set_xlabel(r\"$Y_1$\", fontsize=20)\n",
    "ax[2].set_xlabel(r\"$Y_2$\", fontsize=20)\n",
    "ax[0].set_ylabel(\"Probability Density\", fontsize=20)\n",
    "ax[0].tick_params(axis=\"both\", labelsize=14)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train W model\n",
    "\n",
    "We train a neural network (NN) model to learn the W function, which represents the ratio of the response kernel parametrized by $\\theta$ to the Monte Carlo (MC) kernel, i.e. $w(y,x,\\theta)=p(y|x,\\theta)/q(y|x)$.\n",
    "\n",
    "In this example, we know that the response kernel follows a Gaussian distribution, meaning the analytic form of the W function is also known. As a result, training the W model is not necessary to run the ProfileOmniFold algorithm. Instead, we can use the true W function `w_func` as a direct alternative in the next section if preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training the neural network\n",
    "config = {\n",
    "    'batch_size': 10000,\n",
    "    'lr': 0.001,\n",
    "    'patience': 10,\n",
    "    'activation': nn.ReLU()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic Data (varying theta, used for training W function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsys = 100000\n",
    "# simulation with theta variation\n",
    "theta_min = 0.5\n",
    "theta_max = 2.0\n",
    "\n",
    "theta0_sim = np.random.uniform(theta_min, theta_max, Nsys).reshape(-1, 1)\n",
    "theta1_sim = np.random.uniform(theta_min, theta_max, Nsys).reshape(-1, 1)\n",
    "\n",
    "thetas = theta1_sim[:,0]\n",
    "\n",
    "x_sys = np.random.normal(loc=mu_mc,scale=sigma_mc,size=Nsys).reshape(-1,1)\n",
    "\n",
    "y_nominal1 = np.random.normal(loc=x_sys[:,0],scale=sigma1_kernel_mc,size=Nsys).reshape(-1,1)\n",
    "y_nominal2 = np.random.normal(loc=x_sys[:,0],scale=sigma2_kernel_mc,size=Nsys).reshape(-1,1)\n",
    "y_nominal = np.hstack([y_nominal1,y_nominal2])\n",
    "\n",
    "y_sys1 = np.random.normal(loc=x_sys[:,0],scale=sigma1_kernel_mc,size=Nsys).reshape(-1,1)\n",
    "y_sys2 = []\n",
    "for i in range(Nsys):\n",
    "    y_sys2.append(np.random.normal(x_sys[i,0], theta1_sim[i,0]))\n",
    "y_sys2 = np.array(y_sys2).reshape(-1,1)\n",
    "y_sys = np.hstack([y_sys1,y_sys2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a single W function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into W dataset, data loaders and create loss function\n",
    "w_ds = pof.w_dataset(x_mc, y_mc, theta0_sim, x_sys, y_sys, theta1_sim)\n",
    "\n",
    "# split samples to 50% train and 50% test sets\n",
    "w_ds_train, w_ds_test = random_split(w_ds, [len(w_ds)//2, len(w_ds)-len(w_ds)//2])\n",
    "w_dataloader_train = DataLoader(w_ds_train, batch_size=100000, shuffle=True)\n",
    "w_dataloader_test = DataLoader(w_ds_test, batch_size=100000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train W model\n",
    "\n",
    "wRT_model_network = pof.wRT_network(sigmoid=True, n_inputs=4, activation=config['activation']).double().to(dvc)\n",
    "optimizerRT = optim.Adam(wRT_model_network.parameters(), lr=config['lr'])\n",
    "loss_fn_RT = nn.BCELoss()\n",
    "wRT_tr = pof.w_trainer(w_dataloader_train, w_dataloader_test, wRT_model_network, loss_fn_RT, optimizerRT, patience=config['patience'])\n",
    "\n",
    "wT_model_network = pof.wT_network(sigmoid=True, n_inputs=2, activation=config['activation']).double().to(dvc)\n",
    "optimizerT = optim.Adam(wT_model_network.parameters(), lr=config['lr'])\n",
    "loss_fn_T = nn.BCELoss()\n",
    "wT_tr = pof.w_trainer(w_dataloader_train, w_dataloader_test, wT_model_network, loss_fn_T, optimizerT)\n",
    "\n",
    "wRT_tr.fit()\n",
    "wT_tr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, save the models for later access\n",
    "wRT_checkpoint = {\n",
    "    \"model_state_dict\": wRT_model_network.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"num_sys\": Nsys,\n",
    "    \"num_mc\": Nsim,\n",
    "    \"theta_min\": theta_min,\n",
    "    \"theta_max\": theta_max\n",
    "}\n",
    "wT_checkpoint = {\n",
    "    \"model_state_dict\": wT_model_network.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"num_sys\": Nsys,\n",
    "    \"num_mc\": Nsim,\n",
    "    \"theta_min\": theta_min,\n",
    "    \"theta_max\": theta_max\n",
    "}\n",
    "\n",
    "torch.save(wRT_checkpoint, \"models/2DGaussian/wRT_network_2dgaussian.pth\")\n",
    "torch.save(wT_checkpoint, \"models/2DGaussian/wT_network_2dgaussian.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an ensemble of W functions\n",
    "\n",
    "Training an ensemble of W functions can help to reduce the model uncertainty in the ProfileOmniFold algorithm. In this section, we train multiple W models and save them for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_ensemble = 10\n",
    "# Theta range for systematic data\n",
    "theta_min = 0.5\n",
    "theta_max = 2.0\n",
    "Nsys = 100000\n",
    "Nsim = 100000\n",
    "\n",
    "\n",
    "for i in range(1,num_ensemble+1):\n",
    "    print(\"model \", i, \"training...\")\n",
    "    x_mc = np.random.normal(loc=mu_mc,scale=sigma_mc,size=Nsim).reshape(-1,1)\n",
    "    y_mc1 = np.random.normal(loc=x_mc[:,0]+mu_kernel_mc,scale=sigma1_kernel_mc,size=Nsim).reshape(-1,1)\n",
    "    y_mc2 = np.random.normal(loc=x_mc[:,0]+mu_kernel_mc,scale=sigma2_kernel_mc,size=Nsim).reshape(-1,1)\n",
    "    y_mc = np.hstack([y_mc1,y_mc2])\n",
    "    \n",
    "    theta0_sim = np.random.uniform(theta_min, theta_max, Nsys).reshape(-1, 1)\n",
    "    theta1_sim = np.random.uniform(theta_min, theta_max, Nsys).reshape(-1, 1)\n",
    "    \n",
    "    thetas = theta1_sim[:,0]\n",
    "    \n",
    "    x_sys = np.random.normal(loc=mu_mc,scale=sigma_mc,size=Nsys).reshape(-1,1)\n",
    "    \n",
    "    y_nominal1 = np.random.normal(loc=x_sys[:,0],scale=sigma1_kernel_mc,size=Nsys).reshape(-1,1)\n",
    "    y_nominal2 = np.random.normal(loc=x_sys[:,0],scale=sigma2_kernel_mc,size=Nsys).reshape(-1,1)\n",
    "    y_nominal = np.hstack([y_nominal1,y_nominal2])\n",
    "    \n",
    "    y_sys1 = np.random.normal(loc=x_sys[:,0],scale=sigma1_kernel_mc,size=Nsys).reshape(-1,1)\n",
    "    y_sys2 = []\n",
    "    for j in range(Nsys):\n",
    "        y_sys2.append(np.random.normal(x_sys[j,0], theta1_sim[j,0]))\n",
    "    y_sys2 = np.array(y_sys2).reshape(-1,1)\n",
    "    y_sys = np.hstack([y_sys1,y_sys2])\n",
    "    \n",
    "    # Convert data into W dataset, data loaders and create loss function\n",
    "    w_ds = pof.w_dataset(x_mc, y_mc, theta0_sim, x_sys, y_sys, theta1_sim)\n",
    "    \n",
    "    # split samples to 50% train and 50% test sets\n",
    "    w_ds_train, w_ds_test = random_split(w_ds, [len(w_ds)//2, len(w_ds)-len(w_ds)//2])\n",
    "    w_dataloader_train = DataLoader(w_ds_train, batch_size=100000, shuffle=True)\n",
    "    w_dataloader_test = DataLoader(w_ds_test, batch_size=100000, shuffle=False)\n",
    "    \n",
    "    # Train W model\n",
    "    \n",
    "    wRT_model_network = pof.wRT_network(sigmoid=True, n_inputs=4, activation=config['activation']).double().to(dvc)\n",
    "    optimizerRT = optim.Adam(wRT_model_network.parameters(), lr=config['lr'])\n",
    "    loss_fn_RT = nn.BCELoss()\n",
    "    wRT_tr = pof.w_trainer(w_dataloader_train, w_dataloader_test, wRT_model_network, loss_fn_RT, optimizerRT, patience=config['patience'])\n",
    "    \n",
    "    wT_model_network = pof.wT_network(sigmoid=True, n_inputs=2, activation=config['activation']).double().to(dvc)\n",
    "    optimizerT = optim.Adam(wT_model_network.parameters(), lr=config['lr'])\n",
    "    loss_fn_T = nn.BCELoss()\n",
    "    wT_tr = pof.w_trainer(w_dataloader_train, w_dataloader_test, wT_model_network, loss_fn_T, optimizerT)\n",
    "    \n",
    "    wRT_tr.fit()\n",
    "    wT_tr.fit()\n",
    "\n",
    "    wRT_checkpoint = {\n",
    "        \"model_state_dict\": wRT_model_network.state_dict(),\n",
    "        \"config\": config,\n",
    "        \"num_sys\": x_sys.shape[0],\n",
    "        \"num_mc\": x_mc.shape[0],\n",
    "        \"theta_min\": theta_min,\n",
    "        \"theta_max\": theta_max\n",
    "    }\n",
    "    wT_checkpoint = {\n",
    "        \"model_state_dict\": wT_model_network.state_dict(),\n",
    "        \"config\": config,\n",
    "        \"num_sys\": x_sys.shape[0],\n",
    "        \"num_mc\": x_mc.shape[0],\n",
    "        \"theta_min\": theta_min,\n",
    "        \"theta_max\": theta_max\n",
    "    }\n",
    "        \n",
    "    torch.save(wRT_checkpoint, f\"models/2DGaussian/Ensemble/wRT_network_2dgaussian({i}).pth\")\n",
    "    torch.save(wT_checkpoint, f\"models/2DGaussian/Ensemble/wT_network_2dgaussian({i}).pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load W model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the models if saved previously\n",
    "wRT_model_network = pof.wRT_network(sigmoid=True, n_inputs=4).double().to(dvc)\n",
    "wT_model_network = pof.wT_network(sigmoid=True, n_inputs=2).double().to(dvc)\n",
    "\n",
    "wRT_model_network.load_state_dict(torch.load(\"models/2DGaussian/Ensemble/wRT_network_2dgaussian(1).pth\")[\"model_state_dict\"])\n",
    "wT_model_network.load_state_dict(torch.load(\"models/2DGaussian/Ensemble/wT_network_2dgaussian(1).pth\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ensemble\n",
    "# natsorted is not needed. It was just to list the files in order\n",
    "wRT_list = natsorted(glob.glob(\"models/2DGaussian/Ensemble/wRT_network_2dgaussian(*).pth\"))\n",
    "wT_list = natsorted(glob.glob(\"models/2DGaussian/Ensemble/wT_network_2dgaussian(*).pth\"))\n",
    "\n",
    "wRT_ensemble = []\n",
    "wT_ensemble = []\n",
    "for i in range(len(wRT_list)):\n",
    "    wRT_ensemble.append(pof.wRT_network(sigmoid=True, n_inputs=4).double().to(dvc))\n",
    "    wT_ensemble.append(pof.wT_network(sigmoid=True, n_inputs=2).double().to(dvc))\n",
    "    wRT_ensemble[i].load_state_dict(torch.load(wRT_list[i])[\"model_state_dict\"])\n",
    "    wT_ensemble[i].load_state_dict(torch.load(wT_list[i])[\"model_state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the w function on the MC dataset (so that it becomes only a function of theta)\n",
    "ds = pof.test_dataset(x_mc, y_mc)\n",
    "ds_dataloader = DataLoader(ds, batch_size=100000, shuffle=False)\n",
    "\n",
    "# compute w_theta using single model\n",
    "#w_theta_nn = pof.make_w_theta(ds_dataloader, wRT_model_network, wT_model_network)\n",
    "# compute w_theta using ensemble\n",
    "w_theta_nn_ensemble = pof.make_w_theta_ensemble(ds_dataloader, wRT_ensemble, wT_ensemble, func='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile OmniFold Algorithm\n",
    "\n",
    "Now, let's run the algorithm! There are two options:  \n",
    " \n",
    "- Use the fitted w function, `w_theta_nn` (`w_theta_nn_ensemble`), obtained above.  \n",
    "- Use the true w function, `w_func`, since we know the form of the smearing kernel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the NN solution\n",
    "#w_theta = w_theta_nn\n",
    "w_theta = w_theta_nn_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the true function\n",
    "def w_theta(theta):\n",
    "    return w_func(x_mc, y_mc, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run POF with a single initial theta value\n",
    "theta0 = 1.0\n",
    "pof_out = pof.profile_omnifold(y_data, x_mc, y_mc, iterations=10, w_theta=w_theta, theta_bar=1.0, theta0=theta0, theta_range=[1.0,2.0], num_grid_points=30,\n",
    "                                               no_penalty=True, epochs=20, patience=3, verbose=0)\n",
    "nu_pof = pof_out['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate Ensemble of POF solutions with different initial theta0\n",
    "theta0_list = [i for i in np.arange(1.0, 2.0, 0.1) for _ in range(1)]\n",
    "print('theta0_list:', np.round(theta0_list,1))\n",
    "pof_list = []\n",
    "for theta0 in theta0_list:\n",
    "    res = pof.profile_omnifold(y_data, x_mc, y_mc, iterations=10, w_theta=w_theta, theta_bar=1.0, theta0=theta0, theta_range=[0.5, 1.5], \n",
    "                                                       num_grid_points=50, no_penalty=True, epochs=20, \n",
    "                                                       return_Q=True, return_acc=True, return_loss=True, verbose=0)\n",
    "    pof_list.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated theta in each iteration\n",
    "temp = np.arange(1,11,dtype=int)\n",
    "columns = ['iteration'] + ['theta0: '] * len(pof_list)\n",
    "for i in range(0, len(pof_list)):\n",
    "    temp = np.vstack((temp, pof_list[i]['weights'][:,3,0]))\n",
    "    columns[i+1] += str(np.round(pof_list[i]['theta0'], 1))\n",
    "df_theta = pd.DataFrame(temp.T, columns=columns)\n",
    "\n",
    "\n",
    "# validation accuracy of step 1 neural network\n",
    "temp = np.arange(1,11,dtype=int)\n",
    "columns = ['iteration'] + ['theta0: '] * len(pof_list)\n",
    "for i in range(0, len(pof_list)):\n",
    "    temp = np.vstack((temp, pof_list[i]['step1_val_acc'].ffill(axis=1).iloc[:,-1]))\n",
    "    columns[i+1] += str(np.round(pof_list[i]['theta0'], 1))\n",
    "df_acc = pd.DataFrame(temp.T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta and accuracy evolution\n",
    "\n",
    "# Normalize values to colormap range\n",
    "norm = mcolors.Normalize(vmin=min(theta0_list), vmax=max(theta0_list))\n",
    "cmap = cm.viridis_r\n",
    "\n",
    "# Plot each theta curve\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(10, 8), sharex=True)\n",
    "for i in range(1,len(df_theta.columns)):\n",
    "    ax1.plot([0]+df_theta['iteration'].tolist(), [pof_list[i-1]['theta0']] + df_theta.iloc[:,i].tolist(), \n",
    "             color=cmap(norm(theta0_list[i-1])))\n",
    "\n",
    "\n",
    "ax1.axhline(y=theta, color='red', linestyle='--', linewidth=1.2, label=rf'Truth: $\\theta={theta}$')\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_ylabel(r\"$\\hat{\\theta}$\", fontsize=16)\n",
    "ax1.set_title(r\"$\\hat{\\theta}$ update in EM iteration\", fontsize=16)\n",
    "ax1.tick_params(axis='both', labelsize=14) \n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot each goodness_of_fit curve\n",
    "for i in range(1,len(df_acc.columns)):\n",
    "    ax2.plot(df_theta['iteration'], 1 - np.abs(df_acc.iloc[:,i]-0.5) * 2, color=cmap(norm(theta0_list[i-1])))\n",
    "\n",
    "\n",
    "ax2.set_xlabel(\"Iteration\", fontsize=16)\n",
    "ax2.set_ylabel(\"Goodness of fit\", fontsize=16)\n",
    "ax2.set_title(\"Goodness of fit in EM iteration\", fontsize=16)\n",
    "ax2.tick_params(axis='both', labelsize=14) \n",
    "ax2.grid(True)\n",
    "\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])  # dummy for colorbar\n",
    "cbar = fig.colorbar(sm, ax=[ax1, ax2], orientation='vertical', fraction=0.025, pad=0.02)\n",
    "cbar.set_label(r'$\\theta_0$', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate which iteration to select the best weights from\n",
    "itr = -1\n",
    "\n",
    "# find the best weights (fit) by the classifier accurarcy (closer to 0.5 indicates a better fit)\n",
    "nu_pof, best_run = pof.best_weights(pof_list, itr)\n",
    "pof_out = pof_list[best_run]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"chosen run:\", best_run)\n",
    "print(\"theta0:\", pof_out['theta0'])\n",
    "print(\"fitted theta:\", nu_pof[itr,3,0])\n",
    "print(\"true theta:\", theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As a comparison, we can also run the vanilla omnifold algorithm\n",
    "of_out = pof.omnifold(y_data, x_mc, y_mc, iterations=10, verbose=0, epochs=20)\n",
    "nu_of = of_out['weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate which iteration to plot\n",
    "itr = -1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, sharex=True, figsize=(10, 7))\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=x_data[:, 0], ax=ax,\n",
    "    color=\"black\", linestyle=\"-\", linewidth=2, bw_adjust=2,\n",
    "    label=f\"Experiment\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=x_mc[:, 0], ax=ax,\n",
    "    color=\"tab:blue\", linestyle=\":\", linewidth=2, bw_adjust=2, \n",
    "    label=f\"MC Simulation\"\n",
    ")\n",
    "sns.kdeplot(\n",
    "    x=x_mc[:, 0], ax=ax,\n",
    "    weights=nu_of[itr, 1, :],\n",
    "    color=\"tab:green\", linestyle=\"--\", linewidth=2, bw_adjust=2,\n",
    "    label=rf\"OmniFold ($\\theta=1$)\"\n",
    ")\n",
    "sns.kdeplot(\n",
    "    x=x_mc[:, 0], ax=ax,\n",
    "    weights=nu_pof[itr, 1, :],\n",
    "    color=\"tab:orange\", linestyle=\"-.\", linewidth=2, bw_adjust=2,\n",
    "    label=rf\"Profile OmniFold ($\\hat{{\\theta}}={nu_pof[itr,3,0]:.2f}$)\"\n",
    ")\n",
    "\n",
    "\n",
    "ax.legend(loc=\"best\", fontsize=15, frameon=False)\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_xlabel(r\"$X$\", fontsize=20)\n",
    "ax.set_ylabel(\"Probability Density\", fontsize=20)\n",
    "ax.tick_params(axis=\"both\", labelsize=14)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y1\n",
    "cpwr = utils.comparison_plots_with_ratio(-5, 5, 50, xlabel=r\"$Y_1$\", density=True, header=\"\")\n",
    "cpwr.add_data(y_data[:,0], label=rf\"Experiment ($\\theta={theta}$)\", target=True, histtype=\"step\", color='black', ls=\"-\", lw=2)\n",
    "cpwr.add_data(y_mc[:,0], label=rf\"MC Simulation ($\\theta=1$)\", histtype=\"step\", color='tab:blue', ls=\":\", lw=2)\n",
    "cpwr.add_data(y_mc[:,0], weights=nu_of[itr,0,:], label=rf\"OmniFold ($\\theta=1$)\", histtype=\"step\", color='tab:green', ls=\"--\", lw=2)\n",
    "cpwr.add_data(y_mc[:,0], weights=nu_pof[itr,0,:]*nu_pof[itr,2,:], label=rf'Profile OmniFold ($\\hat{{\\theta}}={nu_pof[itr,3,0]:.2f}$)', histtype=\"step\", color='tab:orange', ls=\"-.\", lw=2)\n",
    "cpwr.show()\n",
    "\n",
    "# Y2\n",
    "cpwr = utils.comparison_plots_with_ratio(-5, 5, 50, xlabel=r\"$Y_2$\", density=True, header=\"\")\n",
    "cpwr.add_data(y_data[:,1], label=rf\"Experiment ($\\theta={theta}$)\", target=True, histtype=\"step\", color='black', ls=\"-\", lw=2)\n",
    "cpwr.add_data(y_mc[:,1], label=rf\"MC Simulation ($\\theta=1$)\", histtype=\"step\", color='tab:blue', ls=\":\", lw=2)\n",
    "cpwr.add_data(y_mc[:,1], weights=nu_of[itr,0,:], label=rf\"OmniFold ($\\theta=1$)\", histtype=\"step\", ls=\"--\", color='tab:green', lw=2)\n",
    "cpwr.add_data(y_mc[:,1], weights=nu_pof[itr,0,:]*nu_pof[itr,2,:], label=rf'Profile OmniFold ($\\hat{{\\theta}}={nu_pof[itr,3,0]:.2f}$)', histtype=\"step\", ls=\"-.\",color='tab:orange', lw=2)\n",
    "cpwr.show()\n",
    "\n",
    "# X\n",
    "cpwr = utils.comparison_plots_with_ratio(-5, 5, 50, xlabel=r\"$X$\", density=True, legend_corner=\"best\", header=\"\")\n",
    "cpwr.add_data(x_data, label=\"Experiment\", target=True, histtype=\"step\", color='black', ls=\"-\", lw=2)\n",
    "cpwr.add_data(x_mc, label=\"MC Simulation\", histtype=\"step\", color='tab:blue', ls=\":\", lw=2)\n",
    "cpwr.add_data(x_mc, weights=nu_of[itr,1,:], label=rf\"OmniFold ($\\theta=1$)\", histtype=\"step\", color='tab:green', ls=\"--\", lw=2)\n",
    "cpwr.add_data(x_mc, weights=nu_pof[itr,1,:], label=rf'Profile OmniFold ($\\hat{{\\theta}}={nu_pof[itr,3,0]:.2f}$)', histtype=\"step\", color='tab:orange', ls=\"-.\",lw=2)\n",
    "cpwr.show()\n",
    "\n",
    "print('fitted theta:', nu_pof[itr,3,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unfolding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
