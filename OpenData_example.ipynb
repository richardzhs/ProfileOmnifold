{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Data Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we consider [CMS open dataset](https://energyflow.network/docs/datasets/) for jets. At the particle level, we consider the sum of the leading and subleading jets as our observable. At the detector level, the observables include both the sum and the difference of the leading and subleading jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "\n",
    "# Energy-flow package for CMS Open Data loader\n",
    "import energyflow as ef\n",
    "from energyflow.utils import remap_pids\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "# POF functions\n",
    "import utils\n",
    "import profile_omnifold as pof\n",
    "\n",
    "dvc = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {dvc} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the nuisance parameter\n",
    "theta = 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cache_dir, pt_lower, pt_upper, eta, quality, pad, x_dim = 3, momentum_scale = 250, n = 100000, amount = 1, max_particle_select = None, frac = 1.0):\n",
    "\n",
    "    # Load data\n",
    "    specs = [f'{pt_lower} <= gen_jet_pts <= {pt_upper}', f'abs_gen_jet_eta < {eta}', f'quality >= {quality}']\n",
    "    # specs = [f'{pt_lower} <= jet_pts <= {pt_upper}', f'abs_jet_eta < {eta}', f'quality >= {quality}']\n",
    "    sim = ef.mod.load(*specs, cache_dir = cache_dir, dataset='sim', amount= amount, store_gens = False, subdatasets=['SIM600_Jet300_pT375-infGeV'])\n",
    "\n",
    "    # Gen_pt for Y\n",
    "    Y1 = sim.jets_f[:,sim.gen_jet_pt]\n",
    "    Y = np.zeros((Y1.shape[0], 1), dtype = np.float32 )\n",
    "    Y[:,0] = Y1 / momentum_scale\n",
    "\n",
    "    # Sim_pt for X\n",
    "    X = np.zeros((Y1.shape[0],3), dtype = np.float32)\n",
    "    event_ids = np.zeros((Y1.shape[0],1), dtype = np.int32)\n",
    "    X[:,0] = sim.jets_f[:,sim.jet_pt] / momentum_scale\n",
    "    X[:,1] = sim.jets_f[:,sim.jet_eta]\n",
    "    X[:,2] = sim.jets_f[:,sim.jet_phi]\n",
    "    event_ids = sim.jets_i[:,sim.evn]\n",
    "\n",
    "    Y = Y[:n]\n",
    "    X = X[:n]\n",
    "    event_ids = event_ids[:n]\n",
    "\n",
    "    # Trim and shuffle\n",
    "    if max_particle_select is not None:\n",
    "        dataset = dataset[particle_counts < max_particle_select]\n",
    "        Y = Y[particle_counts < max_particle_select]\n",
    "        X = X[particle_counts < max_particle_select]\n",
    "\n",
    "    print(\"X: \", X.shape, X.dtype)\n",
    "    print(\"Y: \", Y.shape, Y.dtype)\n",
    "\n",
    "    return X, Y, event_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dim = 1\n",
    "x_dim = 3\n",
    "\n",
    "\n",
    "\n",
    "# Dataset Parameters. Change cache_dir to your local path where the data will be stored.\n",
    "cache_dir = \"Dataset/CMS-Open-Data\"\n",
    "momentum_scale = 1000\n",
    "n = 100000\n",
    "pad = 150\n",
    "pt_lower, pt_upper = 500, 1000\n",
    "eta = 2.4\n",
    "quality = 2\n",
    "\n",
    "# #############################\n",
    "# ########## DATASET ##########\n",
    "# #############################\n",
    "\n",
    "X, Y, ids = load_data(cache_dir, pt_lower, pt_upper, eta, quality, pad, momentum_scale = momentum_scale, n = n, max_particle_select = None, amount = 1)\n",
    "X_test, Y_test, ids_test = load_data(cache_dir, pt_lower, pt_upper, eta, quality, pad, momentum_scale = momentum_scale, n = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted = np.sort(ids)\n",
    "sorted_indices = ids.argsort()\n",
    "print(ids.shape)\n",
    "print(sorted)\n",
    "\n",
    "\n",
    "counter = 0\n",
    "pairs = []\n",
    "N = len(sorted)\n",
    "for (i,id) in enumerate(sorted):\n",
    "    for (j, id2) in enumerate(sorted[(i+1):]):\n",
    "\n",
    "        if id == id2:\n",
    "            counter += 1\n",
    "            pairs.append((i, i+1+j))\n",
    "            break\n",
    "\n",
    "        if id2 > id:\n",
    "            break\n",
    "\n",
    "print(counter / N )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_gen_pts = Y[sorted_indices]\n",
    "sorted_sim_pts = X[sorted_indices,0]\n",
    "\n",
    "leading_gen_pts = []\n",
    "subleading_gen_pts = []\n",
    "leading_sim_pts = []\n",
    "subleading_sim_pts = []\n",
    "\n",
    "for pair in pairs:\n",
    "\n",
    "    jet1 = sorted_gen_pts[pair[0]]\n",
    "    jet2 = sorted_gen_pts[pair[1]]\n",
    "\n",
    "    sim_jet1 = sorted_sim_pts[pair[0]]\n",
    "    sim_jet2 = sorted_sim_pts[pair[1]]\n",
    "\n",
    "    leading_gen_pts.append(jet2)\n",
    "    subleading_gen_pts.append(jet1)\n",
    "    leading_sim_pts.append(sim_jet2)\n",
    "    subleading_sim_pts.append(sim_jet1)\n",
    "\n",
    "leading_gen_pts = np.array(leading_gen_pts)[:,0]\n",
    "subleading_gen_pts = np.array(subleading_gen_pts)[:,0]\n",
    "\n",
    "leading_sim_pts = np.array(leading_sim_pts)\n",
    "subleading_sim_pts = np.array(subleading_sim_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Data (Nature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = leading_gen_pts + subleading_gen_pts\n",
    "x_data = x_data.reshape(-1,1).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "leading_sim_pts_hold = leading_gen_pts + theta*(leading_sim_pts - leading_gen_pts)\n",
    "subleading_sim_pts_hold = subleading_gen_pts + theta*(subleading_sim_pts-subleading_gen_pts)\n",
    "y_data = np.c_[leading_sim_pts_hold + subleading_sim_pts_hold, leading_sim_pts_hold - subleading_sim_pts_hold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Data (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_func(x):\n",
    "    return x[0]*np.exp(x[0])\n",
    "\n",
    "weights = np.apply_along_axis(weight_func, axis=1, arr=x_data)\n",
    "\n",
    "# Normalize weights\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "# Perform weighted sampling\n",
    "N_mc = len(x_data)  # Number of samples to draw\n",
    "idx = np.random.choice(np.arange(len(x_data)), size=N_mc, replace=True, p=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mc = leading_gen_pts[idx] + subleading_gen_pts[idx]\n",
    "x_mc = x_mc.reshape(-1,1).astype(np.float64)\n",
    "y_mc = np.c_[leading_sim_pts[idx] + subleading_sim_pts[idx], leading_sim_pts[idx] - subleading_sim_pts[idx]].astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot both experimental and MC distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=x_data[:, 0], ax=ax[0],\n",
    "    color=\"black\", linestyle=\"-\", linewidth=2, bw_adjust=2,\n",
    "    label=f\"Experiment\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=x_mc[:, 0], ax=ax[0],\n",
    "    color=\"tab:blue\", linestyle=\":\", linewidth=2, bw_adjust=2, \n",
    "    label=f\"MC Simulation\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=y_data[:, 0], ax=ax[1],\n",
    "    color=\"black\", linestyle=\"-\", linewidth=2, bw_adjust=2,\n",
    "    label=f\"Experiment\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=y_mc[:, 0], ax=ax[1],\n",
    "    color=\"tab:blue\", linestyle=\":\", linewidth=2, bw_adjust=2, \n",
    "    label=f\"MC Simulation\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=y_data[:, 1], ax=ax[2],\n",
    "    color=\"black\", linestyle=\"-\", linewidth=2, bw_adjust=2,\n",
    "    label=f\"Experiment\"\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    x=y_mc[:, 1], ax=ax[2],\n",
    "    color=\"tab:blue\", linestyle=\":\", linewidth=2, bw_adjust=2, \n",
    "    label=f\"MC Simulation\"\n",
    ")\n",
    "\n",
    "\n",
    "ax[2].legend(loc=\"upper right\", fontsize=15)\n",
    "ax[0].set_xlim(0.8, 2.0)\n",
    "ax[1].set_xlim(0.8, 2.0)\n",
    "ax[2].set_xlim(-0.5, 0.5)\n",
    "ax[0].set_xlabel(r\"$X$\", fontsize=20)\n",
    "ax[1].set_xlabel(r\"$Y_1$\", fontsize=20)\n",
    "ax[2].set_xlabel(r\"$Y_2$\", fontsize=20)\n",
    "ax[0].set_ylabel(\"Probability Density\", fontsize=20)\n",
    "ax[0].tick_params(axis=\"both\", labelsize=14)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to reweight the MC density of $X$ (in <span style=\"color:blue\">blue</span>) to match the unknown density of data $X$ (in <span style=\"color:black\">black</span>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train W model\n",
    "\n",
    "We train a neural network (NN) model to learn the W function, which represents the ratio of the response kernel parametrized by $\\theta$ to the Monte Carlo (MC) kernel, i.e. $w(y,x,\\theta)=p(y|x,\\theta)/q(y|x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training the neural network\n",
    "config = {\n",
    "    'batch_size': 10000,\n",
    "    'lr': 0.001,\n",
    "    'patience': 30,\n",
    "    'activation': nn.ReLU()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic Data (varying theta, used for training W function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the range of training theta\n",
    "theta_min = 0.5\n",
    "theta_max = 2.0\n",
    "N_sys = x_mc.shape[0]\n",
    "\n",
    "theta0_sim = np.random.uniform(theta_min, theta_max, N_mc).reshape(-1,1)\n",
    "theta1_sim = np.random.uniform(theta_min, theta_max, N_sys).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "leading_sim_pts_hold = leading_gen_pts + theta1_sim[:,0]*(leading_sim_pts - leading_gen_pts)\n",
    "subleading_sim_pts_hold = subleading_gen_pts + theta1_sim[:,0]*(subleading_sim_pts-subleading_gen_pts)\n",
    "y_sys = np.c_[leading_sim_pts_hold + subleading_sim_pts_hold, leading_sim_pts_hold - subleading_sim_pts_hold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a single W function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ds = pof.w_dataset(x_mc, y_mc, theta0_sim, x_data, y_sys, theta1_sim)\n",
    "\n",
    "w_ds_train, w_ds_test = random_split(w_ds, [len(w_ds)//2, len(w_ds)-len(w_ds)//2])\n",
    "w_dataloader_train = DataLoader(w_ds_train, batch_size=10000, shuffle=True, num_workers=0)\n",
    "w_dataloader_test = DataLoader(w_ds_test, batch_size=10000, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train W model\n",
    "\n",
    "wRT_model_network = pof.wRT_network(sigmoid=True, n_inputs=4, activation=config['activation']).double().to(dvc)\n",
    "optimizerRT = optim.Adam(wRT_model_network.parameters(), lr=config['lr'])\n",
    "loss_fn_RT = nn.BCELoss()\n",
    "wRT_tr = pof.w_trainer(w_dataloader_train, w_dataloader_test, wRT_model_network, loss_fn_RT, optimizerRT, patience=config['patience'])\n",
    "\n",
    "wT_model_network = pof.wT_network(sigmoid=True, n_inputs=2, activation=config['activation']).double().to(dvc)\n",
    "optimizerT = optim.Adam(wT_model_network.parameters(), lr=config['lr'])\n",
    "loss_fn_T = nn.BCELoss()\n",
    "wT_tr = pof.w_trainer(w_dataloader_train, w_dataloader_test, wT_model_network, loss_fn_T, optimizerT)\n",
    "\n",
    "wRT_tr.fit()\n",
    "wT_tr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, save the models for later access\n",
    "wRT_checkpoint = {\n",
    "    \"model_state_dict\": wRT_model_network.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"num_sys\": N_sys,\n",
    "    \"num_mc\": N_mc,\n",
    "    \"theta_min\": theta_min,\n",
    "    \"theta_max\": theta_max\n",
    "}\n",
    "wT_checkpoint = {\n",
    "    \"model_state_dict\": wT_model_network.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"num_sys\": N_sys,\n",
    "    \"num_mc\": N_mc,\n",
    "    \"theta_min\": theta_min,\n",
    "    \"theta_max\": theta_max\n",
    "}\n",
    "\n",
    "torch.save(wRT_checkpoint, \"models/OpenData/wRT_network_opendata.pth\")\n",
    "torch.save(wT_checkpoint, \"models/OpenData/wT_network_opendata.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an ensemble of W functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    # MC data\n",
    "    N_mc = len(x_data)  # Number of samples to draw\n",
    "    idx = np.random.choice(np.arange(len(x_data)), size=N_mc, replace=True, p=weights)\n",
    "    x_mc = np.c_[leading_gen_pts[idx] + subleading_gen_pts[idx], leading_gen_pts[idx] - subleading_gen_pts[idx]].astype(np.float64)\n",
    "    y_mc = np.c_[leading_sim_pts[idx] + subleading_sim_pts[idx], leading_sim_pts[idx] - subleading_sim_pts[idx]].astype(np.float64)\n",
    "\n",
    "    # Systematic data\n",
    "    theta_min = 0.5\n",
    "    theta_max = 1.5\n",
    "    N_sys = x_mc.shape[0]\n",
    "    \n",
    "    theta0_sim = np.random.uniform(theta_min, theta_max, N_mc).reshape(-1,1)\n",
    "    theta1_sim = np.random.uniform(theta_min, theta_max, N_sys).reshape(-1,1)\n",
    "    \n",
    "    leading_sim_pts_hold = leading_gen_pts + theta1_sim[:,0]*(leading_sim_pts - leading_gen_pts)\n",
    "    subleading_sim_pts_hold = subleading_gen_pts + theta1_sim[:,0]*(subleading_sim_pts-subleading_gen_pts)\n",
    "    Y_sys = np.c_[leading_sim_pts_hold + subleading_sim_pts_hold, leading_sim_pts_hold - subleading_sim_pts_hold]\n",
    "    \n",
    "    w_ds = pof.w_dataset(x_mc[:,0:1], y_mc, theta0_sim, x_data[:,0:1], Y_sys, theta1_sim)\n",
    "    \n",
    "    w_ds_train, w_ds_test = random_split(w_ds, [len(w_ds)//2, len(w_ds)-len(w_ds)//2])\n",
    "    w_dataloader_train = DataLoader(w_ds_train, batch_size=10000, shuffle=True, num_workers=0)\n",
    "    w_dataloader_test = DataLoader(w_ds_test, batch_size=10000, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Train W model\n",
    "    \n",
    "    wRT_model_network = pof.wRT_network(sigmoid=True, n_inputs=4, activation=config['activation']).double().to(dvc)\n",
    "    optimizerRT = optim.Adam(wRT_model_network.parameters(), lr=config['lr'])\n",
    "    loss_fn_RT = nn.BCELoss()\n",
    "    wRT_tr = pof.w_trainer(w_dataloader_train, w_dataloader_test, wRT_model_network, loss_fn_RT, optimizerRT, patience=config['patience'])\n",
    "    \n",
    "    wT_model_network = pof.wT_network(sigmoid=True, n_inputs=2, activation=config['activation']).double().to(dvc)\n",
    "    optimizerT = optim.Adam(wT_model_network.parameters(), lr=config['lr'])\n",
    "    loss_fn_T = nn.BCELoss()\n",
    "    wT_tr = pof.w_trainer(w_dataloader_train, w_dataloader_test, wT_model_network, loss_fn_T, optimizerT)\n",
    "    \n",
    "    wRT_tr.fit()\n",
    "    wT_tr.fit()\n",
    "    \n",
    "    torch.save(wRT_model_network.state_dict(), f\"models/OpenData/Ensemble/wRT_network_opendata({i})\")\n",
    "    torch.save(wT_model_network.state_dict(), f\"models/OpenData/Ensemble/wT_network_opendata({i})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load W model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models if saved previously\n",
    "wRT_model_network = pof.wRT_network(sigmoid=True, n_inputs=4).double().to(dvc)\n",
    "wT_model_network = pof.wT_network(sigmoid=True, n_inputs=2).double().to(dvc)\n",
    "\n",
    "wRT_model_network.load_state_dict(torch.load(\"models/OpenData/wRT_network_opendata.pth\")[\"model_state_dict\"])\n",
    "wT_model_network.load_state_dict(torch.load(\"models/OpenData/wT_network_opendata.pth\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ensemble of models\n",
    "wRT_list = glob.glob(\"models/OpenData/Ensemble/wRT_network_opendata(*)\")\n",
    "wT_list = glob.glob(\"models/OpenData/Ensemble/wT_network_opendata(*)\")\n",
    "\n",
    "wRT_ensemble = []\n",
    "wT_ensemble = []\n",
    "for i in range(len(wRT_list)):\n",
    "    wRT_ensemble.append(pof.wRT_network(sigmoid=True, activation=config['activation'], n_inputs=4).double().to(dvc))\n",
    "    wT_ensemble.append(pof.wT_network(sigmoid=True, activation=config['activation'], n_inputs=2).double().to(dvc))\n",
    "    wRT_ensemble[i].load_state_dict(torch.load(wRT_list[i]))\n",
    "    wT_ensemble[i].load_state_dict(torch.load(wT_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the w function on the MC dataset (so that it becomes only a function of theta)\n",
    "ds = pof.test_dataset(x_mc, y_mc)\n",
    "ds_dataloader = DataLoader(ds, batch_size=100000, shuffle=False)\n",
    "#w_theta_nn = pof.make_w_theta(ds_dataloader, wRT_model_network, wT_model_network)\n",
    "w_theta_nn_ensemble = pof.make_w_theta_ensemble(ds_dataloader, wRT_ensemble, wT_ensemble, func='median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile OmniFold Algorithm\n",
    "\n",
    "Now, let's run the algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the NN fitted w function\n",
    "#w_theta = w_theta_nn\n",
    "w_theta = w_theta_nn_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run POF with a single initial theta value\n",
    "theta0 = 1.0\n",
    "pof_out = pof.profile_omnifold(y_data, x_mc, y_mc, iterations=10, w_theta=w_theta, theta_bar=1.0, theta0=theta0, theta_range=[1.0,2.0], num_grid_points=30,\n",
    "                                               no_penalty=True, epochs=20, patience=3, verbose=0)\n",
    "nu_pof = pof_out['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate Ensemble of POF solutions with different initial theta0\n",
    "theta0_list = np.arange(1.0, 1.9, 0.1)\n",
    "print('theta0_list:', np.round(theta0_list,1))\n",
    "pof_list = []\n",
    "for theta0 in theta0_list:\n",
    "    res = pof.profile_omnifold(y_data, x_mc, y_mc, iterations=10, w_theta=w_theta, theta_bar=1.0, theta0=theta0, theta_range=[0.5, 1.5], \n",
    "                                                       num_grid_points=50, no_penalty=True, epochs=20, patience=3,\n",
    "                                                       return_Q=True, return_acc=True, return_loss=True, verbose=0)\n",
    "    pof_list.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated theta in each iteration\n",
    "temp = np.arange(1,11,dtype=int)\n",
    "columns = ['iteration'] + ['theta0: '] * len(pof_list)\n",
    "for i in range(0, len(pof_list)):\n",
    "    temp = np.vstack((temp, pof_list[i]['weights'][:,3,0]))\n",
    "    columns[i+1] += str(np.round(pof_list[i]['theta0'], 1))\n",
    "df_theta = pd.DataFrame(temp.T, columns=columns)\n",
    "\n",
    "# validation accuracy of step 1 neural network\n",
    "temp = np.arange(1,11,dtype=int)\n",
    "columns = ['iteration'] + ['theta0: '] * len(pof_list)\n",
    "for i in range(0, len(pof_list)):\n",
    "    temp = np.vstack((temp, pof_list[i]['step1_val_acc'].ffill(axis=1).iloc[:,-1]))\n",
    "    columns[i+1] += str(np.round(pof_list[i]['theta0'], 1))\n",
    "df_acc = pd.DataFrame(temp.T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta and accuracy evolution\n",
    "\n",
    "# Normalize values to colormap range\n",
    "norm = mcolors.Normalize(vmin=min(theta0_list), vmax=max(theta0_list))\n",
    "cmap = cm.viridis_r\n",
    "\n",
    "# Plot each theta curve\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(10, 8), sharex=True)\n",
    "for i in range(1,len(df_theta.columns)):\n",
    "    ax1.plot([0]+df_theta['iteration'].tolist(), [pof_list[i-1]['theta0']] + df_theta.iloc[:,i].tolist(), \n",
    "             color=cmap(norm(theta0_list[i-1])))\n",
    "\n",
    "\n",
    "ax1.axhline(y=theta, color='red', linestyle='--', linewidth=1.2, label=rf'Truth: $\\theta={theta}$')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_ylabel(r\"$\\hat{\\theta}$\", size=16)\n",
    "ax1.set_title(r\"$\\hat{\\theta}$ update in EM iteration\", size=16)\n",
    "ax1.tick_params(axis='both', labelsize=14) \n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot each goodness_of_fit curve\n",
    "for i in range(1,len(df_acc.columns)):\n",
    "    ax2.plot(df_theta['iteration'], 1 - np.abs(df_acc.iloc[:,i]-0.5) * 2, color=cmap(norm(theta0_list[i-1])))\n",
    "\n",
    "\n",
    "ax2.set_xlabel(\"Iteration\", size=16)\n",
    "ax2.set_ylabel(\"Goodness of fit\", size=14)\n",
    "ax2.set_title(\"Goodness of fit in EM iteration\", size=16)\n",
    "ax2.tick_params(axis='both', labelsize=14) \n",
    "ax2.grid(True)\n",
    "\n",
    "\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])  # dummy for colorbar\n",
    "cbar = fig.colorbar(sm, ax=[ax1, ax2], orientation='vertical', fraction=0.025, pad=0.02)\n",
    "cbar.set_label(r'$\\theta_0$', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate which iteration to plot\n",
    "itr = -1\n",
    "\n",
    "# find the best weights (fit) by the classifier accurarcy (closer to 0.5 indicates a better fit)\n",
    "nu_pof, best_run = pof.best_weights(pof_list, itr)\n",
    "pof_out = pof_list[best_run]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"chosen run:\", best_run)\n",
    "print(\"theta0:\", pof_out['theta0'])\n",
    "print(\"fitted theta:\", nu_pof[itr,3,0])\n",
    "print(\"true theta:\", theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As a comparison, we can also run the vanilla omnifold algorithm\n",
    "of_out = pof.omnifold(y_data, x_mc, y_mc, iterations=10, verbose=0, epochs=20)\n",
    "nu_of = of_out['weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate which iteration to plot\n",
    "itr = -1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, sharex=True, figsize=(10, 7))\n",
    "\n",
    "sns.kdeplot(x=x_data[:,0], ax=ax, color=\"black\",label=rf'Experiment', linestyle=\"-\", bw_adjust=2, lw=2)\n",
    "sns.kdeplot(x=x_mc[:,0], ax=ax, color=\"tab:blue\",label=rf'MC Simulation', linestyle=\":\", bw_adjust=2, lw=2)\n",
    "sns.kdeplot(x=x_mc[:,0], ax=ax, color=\"tab:green\", weights=nu_of[itr,1,:], label=rf\"OmniFold ($\\theta=1$)\", linestyle=\"--\", bw_adjust=2, lw=2)\n",
    "sns.kdeplot(x=x_mc[:,0], ax=ax, color=\"tab:orange\", weights=nu_pof[itr,1,:],label=rf\"Profile OmniFold ($\\hat{{\\theta}}={nu_pof[itr,3,0]:.2f}$)\", linestyle=\"-.\", bw_adjust=2, lw=2)\n",
    "\n",
    "\n",
    "\n",
    "ax.legend(loc=\"best\", fontsize=15, frameon=False)\n",
    "ax.set_xlim(0.8, 2.0)\n",
    "ax.set_xlabel(r\"$X$ [TeV]\", fontsize=20)\n",
    "ax.set_ylabel(\"Probability Density\", fontsize=20)\n",
    "ax.tick_params(axis=\"both\", labelsize=14)\n",
    "fig.tight_layout()\n",
    "plt.legend(loc='best',fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y1\n",
    "cpwr = utils.comparison_plots_with_ratio(0.8, 2.0, 50, xlabel=r\"$Y_1$ [TeV]\", density=True, legend_corner=\"best\", header=\"\")\n",
    "cpwr.add_data(y_data[:,0], label=rf\"Experiment ($\\theta={theta}$)\", target=True, color='black', histtype=\"step\", ls=\"-\", lw=2)\n",
    "cpwr.add_data(y_mc[:,0], label=rf\"MC Simulation ($\\theta=1$)\", histtype=\"step\", ls=\":\", lw=2, color='tab:blue')\n",
    "cpwr.add_data(y_mc[:,0], weights=nu_of[itr,0,:], label=r\"OmniFold ($\\theta=1$)\", histtype=\"step\", ls=\"--\", lw=2, color='tab:green')\n",
    "cpwr.add_data(y_mc[:,0], weights=nu_pof[itr,0,:]*nu_pof[itr,2,:], label=rf'ProfileOmniFold ($\\hat{{\\theta}}={nu_pof[itr,3,0]:.2f}$)', histtype=\"step\", ls=\"-.\", lw=2, color='tab:orange')\n",
    "cpwr.show()\n",
    "\n",
    "# Y2\n",
    "cpwr = utils.comparison_plots_with_ratio(-0.2, 0.2, 50, xlabel=r\"$Y_2$ [TeV]\", density=True, legend_corner=\"best\", header=\"\")\n",
    "cpwr.add_data(y_data[:,1], label=rf\"Experiment ($\\theta={theta}$)\", target=True, color='black', histtype=\"step\", ls=\"-\", lw=2)\n",
    "cpwr.add_data(y_mc[:,1], label=rf\"MC Simulation ($\\theta=1$)\", histtype=\"step\", ls=\":\", lw=2, color='tab:blue')\n",
    "cpwr.add_data(y_mc[:,1], weights=nu_of[itr,0,:], label=r\"OmniFold ($\\theta=1$)\", histtype=\"step\", ls=\"--\", lw=2, color='tab:green')\n",
    "cpwr.add_data(y_mc[:,1], weights=nu_pof[itr,0,:]*nu_pof[itr,2,:], label=rf'ProfileOmniFold ($\\hat{{\\theta}}={nu_pof[itr,3,0]:.2f}$)', histtype=\"step\", ls=\"-.\", lw=2, color='tab:orange')\n",
    "cpwr.show()\n",
    "\n",
    "# X\n",
    "cpwr = utils.comparison_plots_with_ratio(0.8, 2.0, 50, xlabel=r\"$X$ [TeV]\", density=True, legend_corner=\"best\", header=\"\")\n",
    "cpwr.add_data(x_data, label=\"Experiment\", target=True, color='black', histtype=\"step\", ls=\"-\", lw=2)\n",
    "cpwr.add_data(x_mc, label=\"MC Simulation\", histtype=\"step\", ls=\":\", lw=2, color='tab:blue')\n",
    "cpwr.add_data(x_mc, weights=nu_of[itr,1,:], label=r\"OmniFold ($\\theta=1$)\", histtype=\"step\", ls=\"--\", lw=2, color='tab:green')\n",
    "cpwr.add_data(x_mc, weights=nu_pof[itr,1,:], label=rf'ProfileOmniFold ($\\hat{{\\theta}}={nu_pof[itr,3,0]:.2f}$)', histtype=\"step\", ls=\"-.\", lw=2, color='tab:orange')\n",
    "cpwr.show()\n",
    "\n",
    "print('fitted theta:', nu_pof[itr,3,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
